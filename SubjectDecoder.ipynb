{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce6fb62-2c25-48d8-b690-d3b0445eeae7",
   "metadata": {},
   "source": [
    "## This notebook simply train a complete decoder for a subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551aaac4-0bcd-4592-8ea8-aaff8c421097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hparams in /home/matteo/anaconda3/envs/deep/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: typeguard in /home/matteo/anaconda3/envs/deep/lib/python3.9/site-packages (from hparams) (4.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /home/matteo/anaconda3/envs/deep/lib/python3.9/site-packages (from typeguard->hparams) (6.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /home/matteo/anaconda3/envs/deep/lib/python3.9/site-packages (from typeguard->hparams) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/matteo/anaconda3/envs/deep/lib/python3.9/site-packages (from importlib-metadata>=3.6->typeguard->hparams) (3.16.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4282158a-0ee1-4109-a4a2-5e6193e4c8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/envs/deep/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from hparams import HParams\n",
    "from hps import Hyperparams\n",
    "from vae import VAE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from os.path import join as opj\n",
    "import h5py  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import json\n",
    "from PIL import Image\n",
    "# from diffusers import VersatileDiffusionPipeline\n",
    "# from diffusers import VersatileDiffusionDualGuidedPipeline\n",
    "from diffusers.models import AutoencoderKL, Transformer2DModel, UNet2DConditionModel\n",
    "# from versatile_diffusion_dual_guided import VersatileDiffusionDualGuidedPipeline\n",
    "from versatile_diffusion_dual_guided_fake_images import VersatileDiffusionDualGuidedFromCLIPEmbeddingPipeline\n",
    "from autoencoder import *\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from diffusers.utils import (\n",
    "    randn_tensor,\n",
    ")\n",
    "from decoding import *\n",
    "\n",
    "# from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bd3b2c-9661-4883-b030-f58bb07147de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import diffusers\n",
    "diffusers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c19aee5-6b7c-44b2-8745-d65e368d8a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path=\"/home/matteo/data/NSD\"\n",
    "timeseries_path=opj(base_path,\"nsddata_timeseries\")\n",
    "betas_path=opj(base_path,\"nsddata_betas\")\n",
    "\n",
    "stimuli_path=opj(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\")\n",
    "stim_file_path=opj(stimuli_path,\"nsd_stimuli.hdf5\")\n",
    "sub=\"subj07\"\n",
    "mod=\"func1pt8mm\"\n",
    "subj_data_path=opj(timeseries_path,\"ppdata\",sub,mod,\"timeseries\")\n",
    "subj_betas_path=opj(betas_path,\"ppdata\",sub,mod,\"betas_assumehrf\")\n",
    "\n",
    "subj_betas_roi_extracted_path=opj(base_path,\"processed_roi\",sub,mod)\n",
    "\n",
    "sub_idx=int(sub.split(\"0\")[-1])\n",
    "\n",
    "stim_order_path=opj(base_path,\"nsddata\",\"experiments\",\"nsd\",\"nsd_expdesign.mat\")\n",
    "stim_info_path=opj(base_path,\"nsddata\",\"experiments\",\"nsd\",\"nsd_stim_info_merged.csv\")\n",
    "stim_captions_train_path=opj(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\",\"annotations\",f\"captions_train2017.json\")\n",
    "stim_captions_val_path=opj(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\",\"annotations\",f\"captions_val2017.json\")\n",
    "\n",
    "processed_data=opj(base_path,\"processed_roi\",sub)\n",
    "\n",
    "fmri_train_data=opj(processed_data,f\"MNI/nsd_train_fmriavg_nsdgeneral_sub{sub_idx}.npy\")\n",
    "imgs_train_data=opj(processed_data,f\"MNI/nsd_train_stim_sub{sub_idx}.npy\")\n",
    "captions_train_data=opj(processed_data, f\"MNI/nsd_train_cap_sub{sub_idx}.npy\")\n",
    "       \n",
    "fmri_test_data=opj(processed_data,f\"MNI/nsd_test_fmriavg_nsdgeneral_sub{sub_idx}.npy\")\n",
    "imgs_test_data=opj(processed_data,f\"MNI/nsd_test_stim_sub{sub_idx}.npy\")\n",
    "captions_test_data=opj(processed_data, f\"MNI/nsd_test_cap_sub{sub_idx}.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0448934-3fbc-4f23-89b6-52671f68f7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NSDDataset(Dataset):\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self, fmri_data,imgs_data,caption_data,transforms=None):\n",
    "        self.fmri_data=np.load(fmri_data)\n",
    "        self.imgs_data=np.load(imgs_data).astype(np.uint8)\n",
    "        self.caption_data=np.load(caption_data,allow_pickle=True)\n",
    "        self.transforms=transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return  len(self.fmri_data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        fmri=torch.tensor(self.fmri_data[idx])\n",
    "        img=Image.fromarray(self.imgs_data[idx])\n",
    "        \n",
    "        if self.transforms:\n",
    "            img=self.transforms(img)\n",
    "        \n",
    "        caption=self.caption_data[idx][0] #cambiare se ne voglio altre\n",
    "        \n",
    "        return fmri,img,caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8702c09-60ee-413e-81fb-c23ec8721a6c",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e935bce-d8ca-490c-9461-cfbc6cc9232b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BS=4\n",
    "tr=torchvision.transforms.ToTensor()\n",
    "train_dataset=NSDDataset(fmri_train_data,imgs_train_data,captions_train_data,transforms=tr)\n",
    "test_dataset=NSDDataset(fmri_test_data,imgs_test_data,captions_test_data,transforms=tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1353b8-4869-4976-9d3b-47080470c52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_dataset=torch.utils.data.Subset(train_dataset,indices=[0,1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b89fd57-764f-4196-acf1-b7d8b94868a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_dataloader=DataLoader(small_dataset,BS,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7d7003-39f5-481f-b06d-4c51ee483d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(train_dataset,BS,shuffle=True)\n",
    "test_dataloader=DataLoader(test_dataset,BS,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b96952-39c8-447b-8273-2d39de2da396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x,y,c  = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9773cb60-7f02-4064-9d61-6236f452d344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_pil=torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb93bd3-5484-4bd8-8467-bd002e6ff00e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device=\"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa62f556-85dd-4165-bf32-9105d51c926b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe_embed = VersatileDiffusionDualGuidedPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a9ef5e-4115-4637-8831-69fe89809835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# pipe_embed = VersatileDiffusionDualGuidedFromCLIPEmbeddingPipeline.from_pretrained(\"shi-labs/versatile-diffusion\", )\n",
    "\n",
    "# pipe_embed.remove_unused_weights()\n",
    "\n",
    "# pipe_embed = pipe_embed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45d22c0e-8fb8-4b36-a89c-c62922ef3471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe_embed.vae.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd0fffe9-e7c0-4bae-b60a-d58972fdbbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BrainDiffuserDecoder:\n",
    "    def __init__(self,vae_weights=\"/home/matteo/models/vdvae/vae2.pt\",\n",
    "                 vae_hyper='/home/matteo/models/vdvae/H.sav', \n",
    "                 pretrained=True,\n",
    "                 subj_path=None,\n",
    "                 device=\"cpu\", sub=\"subj02\",save=True):\n",
    "        super().__init__()\n",
    "        self.keep=31\n",
    "        self.device=device\n",
    "        self.pretrained=pretrained\n",
    "        self.subj_path=subj_path\n",
    "        self.sub=sub\n",
    "        \n",
    "        self.shapes={0:(16,1,1),\n",
    "                1: (16, 1, 1),\n",
    "                 2: (16, 4, 4),\n",
    "                 3: (16, 4, 4),\n",
    "                 4: (16, 4, 4),\n",
    "                 5: (16, 4, 4),\n",
    "                 6: (16, 8, 8),\n",
    "                 7: (16, 8, 8),\n",
    "                 8: (16, 8, 8),\n",
    "                 9: (16, 8, 8),\n",
    "                 10: (16, 8, 8),\n",
    "                 11: (16, 8, 8),\n",
    "                 12: (16, 8, 8),\n",
    "                 13: (16, 8, 8),\n",
    "                 14: (16, 16, 16),\n",
    "                 15: (16, 16, 16),\n",
    "                 16: (16, 16, 16),\n",
    "                 17: (16, 16, 16),\n",
    "                 18: (16, 16, 16),\n",
    "                 19: (16, 16, 16),\n",
    "                 20: (16, 16, 16),\n",
    "                 21: (16, 16, 16),\n",
    "                 22: (16, 16, 16),\n",
    "                 23: (16, 16, 16),\n",
    "                 24: (16, 16, 16),\n",
    "                 25: (16, 16, 16),\n",
    "                 26: (16, 16, 16),\n",
    "                 27: (16, 16, 16),\n",
    "                 28: (16, 16, 16),\n",
    "                 29: (16, 16, 16),\n",
    "                 30: (16, 32, 32)}\n",
    "        \n",
    "\n",
    "        print(\"Loading pretrained deep learning backbones\")\n",
    "\n",
    "        with open(vae_hyper, 'rb') as fp:\n",
    "            d = pickle.load(fp)\n",
    "\n",
    "        H=Hyperparams()\n",
    "        for k,v in d.items():\n",
    "            H[k]=v\n",
    "            \n",
    "        vae=VAE(H)    \n",
    "        state_dict = torch.load(vae_weights)\n",
    "        new_state_dict = {}\n",
    "        l = len('module.')\n",
    "        for k in state_dict:\n",
    "            if k.startswith('module.'):\n",
    "                new_state_dict[k[l:]] = state_dict[k]\n",
    "            else:\n",
    "                new_state_dict[k] = state_dict[k]\n",
    "        state_dict = new_state_dict\n",
    "        vae.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "        self.vae=vae.to(device)\n",
    "\n",
    "\n",
    "        self.pipe_embed= VersatileDiffusionDualGuidedFromCLIPEmbeddingPipeline.from_pretrained(\"shi-labs/versatile-diffusion\",)\n",
    "\n",
    "        self.pipe_embed.remove_unused_weights()\n",
    "        self.pipe_embed.to(self.device)\n",
    "        self.transform=torchvision.transforms.Compose([to_pil,torchvision.transforms.Resize(64),torchvision.transforms.ToTensor(),torchvision.transforms.Normalize(mean=110/255,std=69/255)])\n",
    "        \n",
    "    def compute_train_dataset(self,train_dataloader,save=True):\n",
    "        train_fmri=[]\n",
    "        train_imgs=[]\n",
    "        train_captions=[]\n",
    "        train_z={}\n",
    "        train_clip_img_embeds=[]\n",
    "        train_clip_txt_embeds=[]\n",
    "        train_clip_pool_txt=[]\n",
    "        to_pil=torchvision.transforms.ToPILImage()\n",
    "        \n",
    "        first=True\n",
    "        guidance_scale = 7.5\n",
    "        num_images_per_prompt =1\n",
    "        do_classifier_free_guidance = False\n",
    "        keep=self.keep\n",
    "        device=self.device\n",
    "        \n",
    "        for x,y,c in tqdm.tqdm(train_dataloader):\n",
    "\n",
    "            #save fMRI data\n",
    "            train_fmri.append(x)\n",
    "\n",
    "            #save img data\n",
    "            train_imgs.append(y)\n",
    "\n",
    "            train_captions+=list(c)\n",
    "\n",
    "            #encode images in autoencoder and save z representation\n",
    "            with torch.no_grad():\n",
    "                T=torch.stack([self.transform(i) for i in y])\n",
    "                act=self.vae.encoder.forward(T.to(self.device))\n",
    "                px_z, stats = self.vae.decoder.forward(act, get_latents=True)\n",
    "\n",
    "                latents=[i[\"z\"] for i in stats[:keep]]\n",
    "\n",
    "                if first:\n",
    "                    z={k:v.cpu().clamp(-10,10) for k,v in zip(np.arange(keep),latents)}\n",
    "                    train_z.update(z)\n",
    "                    first=False\n",
    "                else:\n",
    "                    z={k:v.cpu().clamp(-10,10) for k,v in zip(np.arange(keep),latents)}\n",
    "\n",
    "                    for k in train_z.keys():\n",
    "                        train_z[k]=torch.cat([train_z[k],z[k]],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #encode images in CLIP\n",
    "                image_features=self.pipe_embed._encode_image_prompt([to_pil(i) for i in y],device=device,num_images_per_prompt=num_images_per_prompt,do_classifier_free_guidance=do_classifier_free_guidance).cpu()\n",
    "                train_clip_img_embeds.append(image_features)\n",
    "\n",
    "                #encode text in clip\n",
    "                text_features=self.pipe_embed._encode_text_prompt(c,device=device,num_images_per_prompt=num_images_per_prompt,do_classifier_free_guidance=do_classifier_free_guidance).cpu()\n",
    "                train_clip_txt_embeds.append(text_features)\n",
    "\n",
    "                #txt pool\n",
    "                # text = clip.tokenize(c).to(device  )\n",
    "                # text_pool_features = model.encode_text(text).cpu()\n",
    "                # train_clip_pool_txt.append(text_pool_features)\n",
    "\n",
    "        train_clip_txt_embeds = torch.cat(train_clip_txt_embeds,axis=0)\n",
    "        train_clip_img_embeds = torch.cat(train_clip_img_embeds,axis=0)\n",
    "\n",
    "        train_fmri = torch.cat(train_fmri,axis=0)\n",
    "        # train_z = torch.cat(train_z,axis=0)  \n",
    "        # train_z={k:torch.cat(v,axis=0) for k,v in train_z.items()}\n",
    "        train_imgs = torch.cat(train_imgs,axis=0)\n",
    "        # train_clip_pool_txt = torch.cat(train_clip_pool_txt,axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "        for k in train_z.keys():\n",
    "            train_z[k]=torch.nan_to_num(train_z[k])\n",
    "\n",
    "        \n",
    "        if save:\n",
    "            sub=self.sub\n",
    "            os.makedirs(f\"models/{sub}\",exist_ok=True)\n",
    "    \n",
    "            ## train\n",
    "            torch.save(train_fmri,f\"models/{sub}/train_fmri.pt\")\n",
    "            torch.save(train_clip_txt_embeds,f\"models/{sub}/train_clip_txt_embeds.pt\")\n",
    "            torch.save(train_clip_img_embeds,f\"models/{sub}/train_clip_img_embeds.pt\")\n",
    "            torch.save(train_imgs,f\"models/{sub}/train_imgs.pt\")\n",
    "            with open(f\"models/{sub}/train_z.sav\",\"wb\") as f:\n",
    "                pickle.dump(train_z,f)\n",
    "\n",
    "            with open(f\"models/{sub}/train_captions.sav\",\"wb\") as f:\n",
    "                pickle.dump(train_captions,f)\n",
    "\n",
    "            print(\"saved training stuff\")\n",
    "        \n",
    "        return train_fmri,train_imgs,train_captions,train_z,train_clip_img_embeds,train_clip_txt_embeds,train_clip_pool_txt\n",
    "    \n",
    "    \n",
    "    # fix nan\n",
    "\n",
    "    \n",
    "    def compute_test_dataset(self,test_dataloader,save=True):\n",
    "        test_fmri=[]\n",
    "        test_imgs=[]\n",
    "        test_captions=[]\n",
    "        test_z={}\n",
    "        test_clip_img_embeds=[]\n",
    "        test_clip_txt_embeds=[]\n",
    "        test_clip_pool_txt=[]\n",
    "        to_pil=torchvision.transforms.ToPILImage()\n",
    "        \n",
    "        first=True\n",
    "        guidance_scale = 7.5\n",
    "        num_images_per_prompt =1\n",
    "        do_classifier_free_guidance = False\n",
    "        keep=self.keep\n",
    "        device=self.device\n",
    "        \n",
    "        for x,y,c in tqdm.tqdm(test_dataloader):\n",
    "\n",
    "            #save fMRI data\n",
    "            test_fmri.append(x)\n",
    "\n",
    "            #save img data\n",
    "            test_imgs.append(y)\n",
    "\n",
    "            test_captions+=list(c)\n",
    "\n",
    "            #encode images in autoencoder and save z representation\n",
    "            with torch.no_grad():\n",
    "                T=torch.stack([self.transform(i) for i in y])\n",
    "                act=self.vae.encoder.forward(T.to(self.device))\n",
    "                px_z, stats = self.vae.decoder.forward(act, get_latents=True)\n",
    "\n",
    "                latents=[i[\"z\"] for i in stats[:keep]]\n",
    "\n",
    "                if first:\n",
    "                    z={k:v.cpu().clamp(-10,10) for k,v in zip(np.arange(keep),latents)}\n",
    "                    test_z.update(z)\n",
    "                    first=False\n",
    "                else:\n",
    "                    z={k:v.cpu().clamp(-10,10) for k,v in zip(np.arange(keep),latents)}\n",
    "\n",
    "                    for k in test_z.keys():\n",
    "                        test_z[k]=torch.cat([test_z[k],z[k]],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #encode images in CLIP\n",
    "                image_features=self.pipe_embed._encode_image_prompt([to_pil(i) for i in y],device=device,num_images_per_prompt=num_images_per_prompt,do_classifier_free_guidance=do_classifier_free_guidance).cpu()\n",
    "                test_clip_img_embeds.append(image_features)\n",
    "\n",
    "                #encode text in clip\n",
    "                text_features=self.pipe_embed._encode_text_prompt(c,device=device,num_images_per_prompt=num_images_per_prompt,do_classifier_free_guidance=do_classifier_free_guidance).cpu()\n",
    "                test_clip_txt_embeds.append(text_features)\n",
    "\n",
    "                #txt pool\n",
    "                # text = clip.tokenize(c).to(device  )\n",
    "                # text_pool_features = model.encode_text(text).cpu()\n",
    "                # test_clip_pool_txt.append(text_pool_features)\n",
    "                \n",
    "        for k in test_z.keys():\n",
    "            test_z[k]=torch.nan_to_num(test_z[k])\n",
    "\n",
    "        test_clip_txt_embeds = torch.cat(test_clip_txt_embeds,axis=0)\n",
    "        test_clip_img_embeds = torch.cat(test_clip_img_embeds,axis=0)\n",
    "\n",
    "        test_fmri = torch.cat(test_fmri,axis=0)\n",
    "        # test_z = torch.cat(test_z,axis=0)  \n",
    "        # test_z={k:torch.cat(v,axis=0) for k,v in test_z.items()}\n",
    "        test_imgs = torch.cat(test_imgs,axis=0)\n",
    "        # test_clip_pool_txt = torch.cat(test_clip_pool_txt,axis=0)\n",
    "        \n",
    "        if save:\n",
    "            sub=self.sub\n",
    "            os.makedirs(f\"models/{sub}\",exist_ok=True)\n",
    "    \n",
    "            ## test\n",
    "            torch.save(test_fmri,f\"models/{sub}/test_fmri.pt\")\n",
    "            torch.save(test_clip_txt_embeds,f\"models/{sub}/test_clip_txt_embeds.pt\")\n",
    "            torch.save(test_clip_img_embeds,f\"models/{sub}/test_clip_img_embeds.pt\")\n",
    "            torch.save(test_imgs,f\"models/{sub}/test_imgs.pt\")\n",
    "            with open(f\"models/{sub}/test_z.sav\",\"wb\") as f:\n",
    "                pickle.dump(test_z,f)\n",
    "\n",
    "            with open(f\"models/{sub}/test_captions.sav\",\"wb\") as f:\n",
    "                pickle.dump(test_captions,f)\n",
    "\n",
    "            print(\"saved testing stuff\")\n",
    "        \n",
    "        return test_fmri,test_imgs,test_captions,test_z,test_clip_img_embeds,test_clip_txt_embeds,test_clip_pool_txt\n",
    "    \n",
    "    \n",
    "    def fit_brain_to_latent(self,train_fmri_norm,train_z):\n",
    "        brain_to_latent ={}\n",
    "        keys=train_z.keys()\n",
    "        alphas=[5e4]*len(keys)\n",
    "        for k,alpha in tqdm.tqdm(list(zip(keys,alphas))):\n",
    "            brain_vdvae_latent=Ridge(alpha, max_iter=10000, fit_intercept=True)\n",
    "            brain_vdvae_latent.fit(train_fmri_norm.numpy(),train_z[k].reshape(train_z[k].shape[0],-1).numpy())\n",
    "            brain_to_latent[k]=brain_vdvae_latent\n",
    "        return brain_to_latent\n",
    "    \n",
    "    def fit_brain_to_img_emb(self,train_fmri_norm,train_clip_img_embeds):\n",
    "        max_len_img=257\n",
    "        brain_to_img_emb=[]\n",
    "\n",
    "        for i in tqdm.tqdm(range(max_len_img)):\n",
    "            m=Ridge(alpha=6e4)\n",
    "            m.fit(train_fmri_norm.numpy(),train_clip_img_embeds[:,i,:].numpy())\n",
    "            brain_to_img_emb.append(m)\n",
    "            \n",
    "        \n",
    "        return brain_to_img_emb\n",
    "    \n",
    "    def fit_brain_to_txt_emb(self,train_fmri_norm,train_clip_txt_embeds):\n",
    "        max_len_txt=77\n",
    "        brain_to_txt_emb=[]\n",
    "\n",
    "        for i in tqdm.tqdm(range(max_len_txt)):\n",
    "            m=Ridge(alpha=1e5)\n",
    "            m.fit(train_fmri_norm.numpy(),train_clip_txt_embeds[:,i,:].numpy())\n",
    "            brain_to_txt_emb.append(m)\n",
    "            \n",
    "        \n",
    "        return brain_to_txt_emb\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def fit(self,train_dataloader,save=True):\n",
    "        \n",
    "        \n",
    "        sub=self.sub\n",
    "        shapes=self.shapes\n",
    "        ## extract latents\n",
    "        print(\"Extracting latent space for training set\")\n",
    "        train_fmri,train_imgs,train_captions,train_z,train_clip_img_embeds,train_clip_txt_embeds,train_clip_pool_txt= self.compute_train_dataset(train_dataloader, save=save)\n",
    "        \n",
    "        self.train_fmri_mean=torch.mean(train_fmri,axis=0)\n",
    "        self.train_fmri_std=torch.std(train_fmri,axis=0)\n",
    "        \n",
    "        train_fmri_norm=(train_fmri-self.train_fmri_mean)/self.train_fmri_std\n",
    "        train_fmri_norm=torch.nan_to_num(train_fmri_norm)\n",
    "        \n",
    "        ## train brain to latent model\n",
    "        print(\"Fit brain to latents model\")\n",
    "        self.brain_to_latent=self.fit_brain_to_latent(train_fmri_norm,train_z)\n",
    "        \n",
    "        print(\"Fit brain to img embeds model\")\n",
    "        self.brain_to_img_emb=self.fit_brain_to_img_emb(train_fmri_norm,train_clip_img_embeds)\n",
    "        \n",
    "        print(\"Fit brain to txt embeds model\")\n",
    "        self.brain_to_txt_emb=self.fit_brain_to_txt_emb(train_fmri_norm,train_clip_txt_embeds)\n",
    "        \n",
    "\n",
    "        \n",
    "        stats={}\n",
    "\n",
    "        ## compute adjusting values\n",
    "        print(\"Computing adjust values\")\n",
    "        \n",
    "        for k,v in self.brain_to_latent.items():\n",
    "            s=shapes[k]\n",
    "            z=torch.tensor(v.predict(train_fmri_norm.numpy())).reshape(-1,*s)\n",
    "\n",
    "            stats[k]={\"mean\":z.mean(0),\"std\":z.std(0)}\n",
    "        \n",
    "\n",
    "                \n",
    "        self.predicted_latent_stats=stats\n",
    "        \n",
    "        latent_adjust_values={}\n",
    "        for i in range(self.keep):\n",
    "            latent_adjust_values[i]={\"mean\":train_z[i].mean(0), \"std\": train_z[i].std(0)}\n",
    "        \n",
    "        self.latent_adjust_values=latent_adjust_values\n",
    "        \n",
    "        \n",
    "        \n",
    "        img_emb=[]\n",
    "        txt_emb=[]\n",
    "\n",
    "        for i in tqdm.tqdm(range(257)):\n",
    "            emb=torch.tensor(self.brain_to_img_emb[i].predict(train_fmri_norm.numpy()))\n",
    "            img_emb.append(emb)\n",
    "\n",
    "\n",
    "        for i in tqdm.tqdm(range(77)):\n",
    "            emb=torch.tensor(self.brain_to_txt_emb[i].predict(train_fmri_norm.numpy()))\n",
    "            txt_emb.append(emb)\n",
    "\n",
    "        img_emb=torch.stack(img_emb,1)\n",
    "        txt_emb=torch.stack(txt_emb,1)\n",
    "        predicted_img_emb_mean=img_emb.mean(0)\n",
    "        predicted_img_emb_std=img_emb.std(0)\n",
    "\n",
    "        predicted_txt_emb_mean=txt_emb.mean(0)\n",
    "        predicted_txt_emb_std=txt_emb.std(0)\n",
    "        \n",
    "        \n",
    "        ## true values\n",
    "        self.clip_img_embeds_mean=train_clip_img_embeds.mean(0)\n",
    "        self.clip_img_embeds_std=train_clip_img_embeds.std(0)\n",
    "\n",
    "\n",
    "        self.clip_txt_embeds_mean=train_clip_txt_embeds.mean(0)\n",
    "        self.clip_txt_embeds_std=train_clip_txt_embeds.std(0)\n",
    "        \n",
    "        self.predicted_img_emb_mean=predicted_img_emb_mean\n",
    "        self.predicted_img_emb_std=predicted_img_emb_std\n",
    "        \n",
    "        self.predicted_txt_emb_mean=predicted_txt_emb_mean\n",
    "        self.predicted_txt_emb_std=predicted_txt_emb_std\n",
    "        \n",
    "\n",
    "        if save:\n",
    "            filename=\"predicted_latent_stats.sav\"\n",
    "\n",
    "            with open(opj(f\"models/{sub}\",filename),\"wb\") as f:\n",
    "                pickle.dump(stats,f)\n",
    "        \n",
    "            filename = f'latent_adjust_values.sav'\n",
    "            with open(opj(f\"models/{sub}\",filename), 'wb') as f:\n",
    "                pickle.dump(latent_adjust_values, f)\n",
    "\n",
    "            # Define the file paths\n",
    "            img_emb_mean_path = f\"models/{sub}/predicted_img_emb_mean.pt\"\n",
    "            img_emb_std_path = f\"models/{sub}/predicted_img_emb_std.pt\"\n",
    "            txt_emb_mean_path = f\"models/{sub}/predicted_txt_emb_mean.pt\"\n",
    "            txt_emb_std_path = f\"models/{sub}/predicted_txt_emb_std.pt\"\n",
    "\n",
    "            # Save the tensors\n",
    "            torch.save(predicted_img_emb_mean, img_emb_mean_path)\n",
    "            torch.save(predicted_img_emb_std, img_emb_std_path)\n",
    "            torch.save(predicted_txt_emb_mean, txt_emb_mean_path)\n",
    "            torch.save(predicted_txt_emb_std, txt_emb_std_path)\n",
    "\n",
    "            torch.save(self.train_fmri_mean,f\"models/{sub}/train_fmri_mean.pt\")\n",
    "            torch.save(self.train_fmri_std,f\"models/{sub}/train_fmri_std.pt\")\n",
    "            \n",
    "            torch.save(self.clip_img_embeds_mean, opj(f\"models/{sub}\",\"clip_img_embeds_mean.pt\"))\n",
    "            torch.save(self.clip_img_embeds_std, opj(f\"models/{sub}\",\"clip_img_embeds_std.pt\"))\n",
    "            torch.save(self.clip_txt_embeds_mean, opj(f\"models/{sub}\",\"clip_txt_embeds_mean.pt\"))\n",
    "            torch.save(self.clip_txt_embeds_std, opj(f\"models/{sub}\",\"clip_txt_embeds_std.pt\"))\n",
    "\n",
    "        \n",
    "        #eventually save models separately\n",
    "        if save:\n",
    "            \n",
    "            print(\"saving all models separately\")\n",
    "            \n",
    "            os.makedirs(f\"models/{sub}/decoding\",exist_ok=True)\n",
    "            for i in train_z.keys():\n",
    "                filename = f'brain_to_vdvae_latent_ridge_{i}.sav'\n",
    "                with open(opj(f\"models/{sub}/decoding\",filename), 'wb') as f:\n",
    "                    pickle.dump(self.brain_to_latent[i], f)\n",
    "\n",
    "            for i in range(257):\n",
    "                filename = f'brain_to_img_emb_ridge_{i}.sav'\n",
    "                with open(opj(f\"models/{sub}/decoding\",filename), 'wb') as f:\n",
    "                    pickle.dump(self.brain_to_img_emb[i], f)\n",
    "\n",
    "            for i in range(77):\n",
    "                filename = f'brain_to_txt_emb_ridge_{i}.sav'\n",
    "                with open(opj(f\"models/{sub}/decoding\",filename), 'wb') as f:\n",
    "                    pickle.dump(self.brain_to_txt_emb[i], f)\n",
    "    \n",
    "        \n",
    "    def get_latents(self,data):\n",
    "        shapes=self.shapes\n",
    "        \n",
    "        adjust=self.latent_adjust_values\n",
    "        latents={}\n",
    "        bs=data.shape[0]\n",
    "        for k,v in self.brain_to_latent.items():\n",
    "            s=shapes[k]\n",
    "            z=torch.tensor(v.predict(data)).reshape(-1,*s)\n",
    "\n",
    "\n",
    "            if adjust is not None and bs>1:\n",
    "                #compute actual mean and std\n",
    "                                \n",
    "                z_mean=self.predicted_latent_stats[k][\"mean\"]  \n",
    "                z_std=self.predicted_latent_stats[k][\"std\"] \n",
    "                \n",
    "                \n",
    "                \n",
    "                #standardize \n",
    "                z = (z - z_mean)/(1e-9+z_std)\n",
    "\n",
    "                #replace with latent mean and std\n",
    "                z = z*adjust[k][\"std\"]+adjust[k][\"mean\"]\n",
    "\n",
    "            latents[k]=z\n",
    "\n",
    "        return latents\n",
    "    \n",
    "    def decode_with_partial_sampling(self,latents,keep=None):\n",
    "        xs = {a.shape[2]: a for a in self.vae.decoder.bias_xs}\n",
    "        \n",
    "        decoder=self.vae.decoder.to(self.device)\n",
    "        out=decoder.forward_manual_latents(keep,latents.values(),t=None)\n",
    "\n",
    "        xs=decoder.out_net.sample(out)\n",
    "        xs=torch.tensor(xs).permute(0,3,1,2)/255\n",
    "        return xs\n",
    "                                             \n",
    "    def decode_features(self,fmri):\n",
    "        \n",
    "        #get latents\n",
    "        z=self.get_latents(fmri.numpy())\n",
    "        \n",
    "        adjust=self.latent_adjust_values\n",
    "        \n",
    "        img_emb=[]\n",
    "        txt_emb=[]\n",
    "        for i in tqdm.tqdm(range(257)):\n",
    "            emb=torch.tensor(self.brain_to_img_emb[i].predict(fmri.numpy()))\n",
    "            # print(emb.shape)\n",
    "            if adjust and len(fmri)>1:\n",
    "                #compute actual mean and std\n",
    "                emb_mean=self.predicted_img_emb_mean[i]\n",
    "                emb_std=self.predicted_img_emb_std[i]\n",
    "\n",
    "                emb= (emb-emb_mean)/emb_std\n",
    "                emb = emb*self.clip_img_embeds_std[i]+self.clip_img_embeds_mean[i]\n",
    "\n",
    "            img_emb.append(emb)\n",
    "\n",
    "        for i in tqdm.tqdm(range(77)):\n",
    "\n",
    "\n",
    "            emb=torch.tensor(self.brain_to_txt_emb[i].predict(fmri.numpy()))\n",
    "\n",
    "            if adjust and len(fmri)>1:\n",
    "                #compute actual mean and std\n",
    "                \n",
    "                emb_mean=self.predicted_txt_emb_mean[i]\n",
    "                emb_std=self.predicted_txt_emb_std[i]\n",
    "                \n",
    "                emb= (emb-emb_mean)/emb_std\n",
    "\n",
    "                emb = emb*self.clip_txt_embeds_std[i]+self.clip_txt_embeds_mean[i]\n",
    "            txt_emb.append(emb)\n",
    "                                             \n",
    "        img_emb=torch.stack(img_emb,1)\n",
    "        txt_emb=torch.stack(txt_emb,1)\n",
    "        \n",
    "        return z, img_emb, txt_emb\n",
    "        \n",
    "        \n",
    "    def reconstruct_guess(self,fmri):\n",
    "        upsample=torchvision.transforms.Resize(512,interpolation=torchvision.transforms.InterpolationMode.BILINEAR)\n",
    "        \n",
    "        z, img_emb, txt_emb = self.decode_features(fmri)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            latents={k:v.to(self.device).float() for k,v in z.items()}\n",
    "            # guess_img=upsample(autoencoder.decoder.double()(z.to(device)).cpu())\n",
    "            guess_img=self.decode_with_partial_sampling(latents=latents,keep=len(fmri))\n",
    "            # img_out=pipe_embed.vae.float().decode(z.float().to(device)).sample.cpu()\n",
    "            print(guess_img.max())\n",
    "            guess_img=upsample(guess_img).clamp(0,1)\n",
    "        \n",
    "        \n",
    "        return guess_img, z, img_emb, txt_emb\n",
    "    \n",
    "    \n",
    "    def decode(self,fmri,strength=7.5,text_to_image_strength=0.4, num_inference_steps=37,how_many=1, use_latents=True, scale=False):\n",
    "        \n",
    "        if scale:\n",
    "            frmi= (fmri- self.train_fmri_mean)/self.train_fmri_std\n",
    "            fmri= torch.nan_to_num(fmri)\n",
    "        \n",
    "        to_pil=torchvision.transforms.ToPILImage()\n",
    "\n",
    "        \n",
    "        # decode initial guess and featuers\n",
    "        guess_img, z, img_emb, txt_emb=self.reconstruct_guess(fmri)\n",
    "        \n",
    "        \n",
    "        # encode null img and null prompt\n",
    "        null_prompt=\"\"\n",
    "        null_img=Image.fromarray(np.zeros((425,425,3),dtype=np.uint8))\n",
    "        uimg=self.pipe_embed._encode_image_prompt([null_img],device=self.device,num_images_per_prompt=1,do_classifier_free_guidance=False).cpu()\n",
    "        utxt=self.pipe_embed._encode_text_prompt([null_prompt],device=self.device,num_images_per_prompt=1,do_classifier_free_guidance=False).cpu()\n",
    "        \n",
    "        \n",
    "        #decode the final images\n",
    "        \n",
    "        scale=self.pipe_embed.vae.config.scaling_factor\n",
    "        images=[]\n",
    "        for i in range(len(fmri)):\n",
    "            with torch.no_grad():\n",
    "                print(f\"[INFO] Final reconstrution {i+1}/{len(fmri)}\")\n",
    "                encoded_latents=scale*self.pipe_embed.vae.encode((2*guess_img[i:i+1]-1).to(self.device)).latent_dist.sample()\n",
    "                noise = randn_tensor((how_many,encoded_latents.shape[1],encoded_latents.shape[2],encoded_latents.shape[3]), device=self.device, dtype=encoded_latents.dtype)\n",
    "                encoded_latents_norm=(encoded_latents-encoded_latents.mean())//(1e-8+encoded_latents.std())\n",
    "                #final_latents=pipe_embed.scheduler.add_noise(0.0*(encoded_latents_norm.clamp(-3,3)),noise,torch.tensor(50).long().to(device))\n",
    "\n",
    "                #final_latents=noise+0.18*encoded_latents_norm.clamp(-3,3)\n",
    "                final_latents=noise+scale*encoded_latents.clamp(-3,3)\n",
    "                final_latents = (final_latents - final_latents.mean())/final_latents.std()\n",
    "                \n",
    "                if use_latents:\n",
    "                    final_latents=noise+scale*encoded_latents.clamp(-3,3)\n",
    "                    final_latents = (final_latents - final_latents.mean())/final_latents.std()\n",
    "                 \n",
    "                else:\n",
    "                    final_latents=noise\n",
    "                \n",
    "\n",
    "                if strength>1:\n",
    "                    txt_cond=torch.cat([utxt.repeat(how_many,1,1),txt_emb[i:i+1].float().repeat(how_many,1,1)],0)\n",
    "\n",
    "                    img_cond=torch.cat([uimg.repeat(how_many,1,1),img_emb[i:i+1].float().repeat(how_many,1,1)],0)\n",
    "                else:\n",
    "                    txt_cond=txt_emb[i:i+1].float().repeat(how_many,1,1)\n",
    "                    img_cond=img_emb[i:i+1].float().repeat(how_many,1,1)\n",
    "\n",
    "                # print(txt_emb[i:i+1].float().repeat(how_many,1,1).shape,img_emb[i:i+1].float().repeat(how_many,1,1).shape,final_latents.shape)\n",
    "\n",
    "                # image_generated = pipe_embed([null_prompt]*bs,guessed,txt_cond.to(device), img_cond.to(device), text_to_image_strength=0.4,num_inference_steps=37,guidance_scale=strength,latents=final_latents).images\n",
    "                image_generated = self.pipe_embed([null_prompt]*how_many,[null_img]*how_many,txt_cond.to(self.device), img_cond.to(self.device), text_to_image_strength=text_to_image_strength,num_inference_steps=num_inference_steps,guidance_scale=strength,latents=final_latents).images\n",
    "                images+=image_generated\n",
    "    \n",
    "        guessed=[to_pil(i) for i in guess_img]\n",
    "        \n",
    "        \n",
    "        return images, guessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49242767-7bcf-44fd-aad4-9419d4f3a37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained deep learning backbones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 17 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 28261.26it/s]\n"
     ]
    }
   ],
   "source": [
    "brain_model=BrainDiffuserDecoder(sub=sub,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f6bb1ac-a308-4405-9404-ff56a125aa00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# brain_model.compute_test_dataset(small_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ace495-b1cd-4b8d-b34a-c98e4d09b6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting latent space for training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2215/2215 [51:37<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved training stuff\n",
      "Fit brain to latents model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [04:17<00:00,  8.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit brain to img embeds model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 257/257 [21:13<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit brain to txt embeds model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [06:00<00:00,  4.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing adjust values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 257/257 [03:02<00:00,  1.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:54<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving all models separately\n"
     ]
    }
   ],
   "source": [
    "brain_model.fit(train_dataloader,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27bdee4c-82fc-4057-98dd-b1c419d88ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x,y,c=next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7071df1a-e000-4375-b89c-6b8ef7b01e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img,gss=brain_model.decode(x2,scale=False,use_latents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da0cd911-7bb5-4bac-9741-1e74d2990d38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine\n"
     ]
    }
   ],
   "source": [
    "print(\"fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4abad509-c499-45b0-a227-f31a3887d5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(y[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d28b9650-b514-4353-ad51-e285c00e3dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x2=(x-brain_model.train_fmri_mean)/brain_model.train_fmri_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80e56232-1848-45ca-9cca-9cf501c3c8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x2=torch.nan_to_num(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95f5de-ae98-41b9-a59c-a810bd01524a",
   "metadata": {},
   "source": [
    "## Check con pretrained Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2e6e681-e48c-452d-afc1-a35e050343a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subj_path=opj(\"/home/matteo/explore_NSD_MNI\",\"models\",sub)\n",
    "\n",
    "# brain_decoder=BrainDiffuserPretrainedDecoder(subj_path=subj_path,device=\"cuda:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
