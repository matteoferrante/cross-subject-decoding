{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b930c4-31f4-4076-bcc0-ad2f3b73a597",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Alignment notebook\n",
    "\n",
    "In this notebook I will align data from different subject to the fMRI data of subj01 using three different methods: anatomical alignment, a ridge regression and the hyperaligment using full and randomized svd\n",
    "\n",
    "- [ ] Subj01 are the target data.\n",
    "- [ ] Using the \"test data\" as alignment data\n",
    "- [ ] Remove from the test set the comparison set\n",
    "- [ ] Clusterize the latent space of CLIP Text Embeddings\n",
    "- [ ] Use 10%, 25%, 50% and 100% of the test dataset for alignment sampling uniformly from the clusters\n",
    "- [ ] Perform all alignment fro subj02, subj05, subj07\n",
    "- [ ] Test reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ca1644-0b05-4f86-86d5-24b3a7c907e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# from nltools.mask import create_sphere, expand_mask\n",
    "# from nltools.data import Brain_Data, Adjacency\n",
    "# from nltools.stats import align\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import warnings\n",
    "# from mvpa2.suite import \n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "import torch\n",
    "from os.path import join as opj\n",
    "from transformers import AutoProcessor, CLIPTextModel\n",
    "import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from hyperalign import align\n",
    "%matplotlib inline\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169e8bce-1fb2-49f9-b0ed-334ca9dd1881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_idx=5\n",
    "target_idx=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c813795b-6246-4b02-ba71-235f01e6ece4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_test_data=f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/MNI/nsd_test_fmriavg_nsdgeneral_sub{source_idx}.npy\"\n",
    "source_train_data=f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/MNI/nsd_train_fmriavg_nsdgeneral_sub{source_idx}.npy\"\n",
    "source_test_imgs=f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/MNI/nsd_test_stim_sub{source_idx}.npy\"\n",
    "\n",
    "target_test_data=f\"/home/matteo/data/NSD/processed_roi/subj0{target_idx}/MNI/nsd_test_fmriavg_nsdgeneral_sub{target_idx}.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394d1d07-d25f-47be-b9f1-e7b32f7bb516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63838196-f6f5-472c-95c0-8a9681d9a115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_imgs=np.load(source_test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff27854-64e0-4b30-8194-35f050540044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_test=np.load(source_test_data)\n",
    "source_train=np.load(source_train_data)\n",
    "\n",
    "\n",
    "target_test=np.load(target_test_data)\n",
    "\n",
    "## Remove useful indices \n",
    "indices=[25,31,68,121,126,318,384,492,531,606,702,860]\n",
    "\n",
    "indices2=[70,116,165,261,278,363,451,774]\n",
    "indices3=[41,205,230,411,428,446,502,777]\n",
    "extra=[95,905]\n",
    "comparison_indices=indices+indices2+indices3+extra\n",
    "\n",
    "align_indices=[i for i in  range(len(source_test)) if i not in comparison_indices]\n",
    "\n",
    "comparison_source=source_test[comparison_indices]\n",
    "\n",
    "## keep only align indices\n",
    "source_test=source_test[align_indices]\n",
    "target_test=target_test[align_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdfed43-bcca-43ff-8633-1fbbab59eb81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 425, 425, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a903f46-34cb-4b3d-a7f7-54773f65f884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_test_NOTALIGNED_fmriavg_nsdgeneral_sub{source_idx}.npy\",source_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af3f6a82-f55c-4d56-a69c-a84ab7dd14f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_comparison_NOTALIGNED_fmriavg_nsdgeneral_sub{source_idx}.npy\",comparison_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84782ec-7cff-4c26-a2d1-bb29474fef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_test_stim_sub{source_idx}.npy\",source_imgs[align_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00a7bf0-9112-442b-8ed4-a192c3bb3142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison_imgs=source_imgs[comparison_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750252c-e3c8-479b-9574-f18fcd77492c",
   "metadata": {},
   "source": [
    "## Load captions and perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6913f17a-4df6-4efb-af33-f4d28b743b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path=\"/home/matteo/brain-diffuser/data\"\n",
    "\n",
    "processed_data=opj(base_path,\"processed_data\",f\"subj0{target_idx}\")\n",
    "captions_test_data=opj(processed_data, f\"nsd_test_cap_sub{target_idx}.npy\")\n",
    "test_captions=np.load(captions_test_data,allow_pickle=True)\n",
    "test_captions=[i[0] for i in test_captions]\n",
    "test_captions=list(np.array(test_captions)[align_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc02e0-bd35-4e12-8c79-dcc72a23e6d0",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b863cf98-f2ad-4c0d-be58-79084f00fb9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda:0\"\n",
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90fe1460-d263-48b3-a5b5-99987c587672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15/15 [00:01<00:00, 14.28it/s]\n"
     ]
    }
   ],
   "source": [
    "mini_batch=64\n",
    "test_clip_txt_cls=[]\n",
    "for i in tqdm.tqdm(range(int(np.ceil(len(test_captions)/mini_batch)))):\n",
    "    with torch.no_grad():\n",
    "        inputs=processor(text=test_captions[mini_batch*i:mini_batch*(i+1)],padding=True,return_tensors=\"pt\").to(device)\n",
    "        outputs=model(**inputs)\n",
    "        test_clip_txt_cls.append(outputs.pooler_output.cpu())\n",
    "test_clip_txt_cls=torch.cat(test_clip_txt_cls,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544f2ff2-ec09-4315-a6ed-1fe365ddeab8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "kmeans=KMeans(n_clusters=6).fit(test_clip_txt_cls.numpy())\n",
    "clusters=kmeans.predict(test_clip_txt_cls.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c94978-61f5-4c87-9093-f63860f55cdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sample indices in a stratified way for different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3593c9e8-7b11-4871-998b-2f39da0c3893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_10_percent,_=train_test_split(np.arange(len(target_test)),train_size=0.1,random_state=42,stratify=clusters)\n",
    "indices_25_percent,_=train_test_split(np.arange(len(target_test)),train_size=0.25,random_state=42,stratify=clusters)\n",
    "indices_50_percent,_=train_test_split(np.arange(len(target_test)),train_size=0.5,random_state=42,stratify=clusters)\n",
    "indices_100_percent=np.arange(len(target_test))\n",
    "\n",
    "percent_indices={\"10\":indices_10_percent,\"25\":indices_25_percent, \"50\": indices_50_percent, \"100\": indices_100_percent}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f673d-afcb-4cb5-8384-c7a1e239b450",
   "metadata": {},
   "source": [
    "## Alignment with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d81559d-13ce-4e36-ba9b-c22c67b96b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:54<00:00, 13.64s/it]\n"
     ]
    }
   ],
   "source": [
    "for k,v in tqdm.tqdm(percent_indices.items()):\n",
    "    aligner=RidgeCV(alphas=[1e2,1e3,1e4,5e4], fit_intercept=True)\n",
    "    aligner.fit(source_test[v],target_test[v])\n",
    "\n",
    "    aligned_source_test=aligner.predict(source_test)\n",
    "    aligned_source_train=aligner.predict(source_train)\n",
    "    aligned_source_comparison=aligner.predict(comparison_source)\n",
    "    \n",
    "    #adjust values\n",
    "    final_mean=target_test.mean(0)\n",
    "    final_std=target_test.std(0)\n",
    "    \n",
    "    \n",
    "    aligned_source_test_adj=(aligned_source_test-aligned_source_test.mean(0))/(1e-8+aligned_source_test.std(0))\n",
    "    aligned_source_test_adj=final_std*aligned_source_test_adj+final_mean\n",
    "\n",
    "    aligned_source_train_adj=(aligned_source_train-aligned_source_train.mean(0))/(1e-8+aligned_source_train.std(0))\n",
    "    aligned_source_train_adj=final_std*aligned_source_train_adj+final_mean\n",
    "\n",
    "    aligned_source_comparison_adj=(aligned_source_comparison-aligned_source_test.mean(0))/(1e-8+aligned_source_test.std(0))\n",
    "    aligned_source_comparison_adj=final_std*aligned_source_comparison_adj+final_mean\n",
    "    \n",
    "    os.makedirs(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}\",exist_ok=True)\n",
    "\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_train_fmriavg_nsdgeneral_sub{source_idx}_ridge_fraction-{k}.npy\",aligned_source_train_adj)\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_test_fmriavg_nsdgeneral_sub{source_idx}_ridge_fraction-{k}.npy\",aligned_source_test_adj)\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_comparison_fmriavg_nsdgeneral_sub{source_idx}_ridge_fraction-{k}.npy\",aligned_source_comparison_adj)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "942faceb-a0c5-471a-af28-c26533db1a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_comparison_stim_sub{source_idx}.npy\",comparison_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51806f98-7105-48af-a7cd-11fcbc142a2e",
   "metadata": {},
   "source": [
    "## Alignment with Hyperalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db23350b-c90d-4787-b3fc-519b469d1241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperAlign:\n",
    "    def __init__(self, device,randomize=False,rank=300):\n",
    "        self.device = device\n",
    "        self.R=None\n",
    "        self.c=None\n",
    "        self.randomize=randomize\n",
    "        self.rank=rank\n",
    "\n",
    "        \n",
    "    def randomized_svd(self,M, rank=None):\n",
    "        M=M.double()\n",
    "        m, n = M.shape\n",
    "\n",
    "        # Step 1: Randomly generate a matrix G\n",
    "        G = torch.randn(n, self.rank).to(M.device).double()\n",
    "\n",
    "        # Step 2: Form a matrix H = MG\n",
    "        H = M @ G\n",
    "\n",
    "        # Step 3: Orthonormalize H\n",
    "        Q, _ = torch.linalg.qr(H)\n",
    "\n",
    "        # Step 4: Form the matrix T = Q.T @ M\n",
    "        T = Q.T @ M\n",
    "\n",
    "        # Step 5: Compute the SVD of T\n",
    "        U_hat, s, V = torch.linalg.svd(T, full_matrices=False)\n",
    "\n",
    "        # Step 6: Compute U = QU_hat\n",
    "        U = Q @ U_hat\n",
    "\n",
    "        return V.T, s, U\n",
    "\n",
    "\n",
    "    def orthogonal_procrustes(self, A, B):\n",
    "        A, B = A.to(self.device), B.to(self.device)\n",
    "\n",
    "        product = A.T @ B\n",
    "        if self.randomize:\n",
    "            U, _, V = self.randomized_svd(product, self.rank)\n",
    "        else:\n",
    "            U, _, V = torch.linalg.svd(product)\n",
    "        R = U @ V.T\n",
    "        c = torch.trace(B.T @ (A @ R)) / torch.trace(A.T @ A)\n",
    "        return R, c\n",
    "\n",
    "    def fit(self,source,target):\n",
    "        R, c = self.orthogonal_procrustes(source, target)\n",
    "        self.R=R\n",
    "        self.c=c\n",
    "        print(\"fit completed\")\n",
    "    \n",
    "    def align(self, source):\n",
    "        \n",
    "        assert self.R is not None, \"Call fit before with source, target before\"\n",
    "        source = source.to(self.device)\n",
    "        # R, c = self.orthogonal_procrustes(source, target)\n",
    "\n",
    "        # Align the source to the target\n",
    "        source_aligned = self.c * source @ self.R\n",
    "\n",
    "        return source_aligned.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65962f6-a4b0-49d6-8fae-1f3a601c86b6",
   "metadata": {},
   "source": [
    "## Random SVD HyperAlignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35669ab4-1b14-44fd-8b10-fe334852ddc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████▎                                 | 1/4 [00:09<00:29,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 2/4 [00:18<00:18,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████▊           | 3/4 [00:26<00:08,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:34<00:00,  8.56s/it]\n"
     ]
    }
   ],
   "source": [
    "for k,v in tqdm.tqdm(percent_indices.items()):\n",
    "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "    hyper_align = HyperAlign(device,randomize=True,rank=4096)\n",
    "\n",
    "    source = torch.tensor(source_test[v])\n",
    "    target = torch.tensor(target_test[v])\n",
    "    hyper_align.fit(source,target)\n",
    "\n",
    "    aligned_source_test=hyper_align.align(torch.tensor(source_test)).numpy()\n",
    "    aligned_source_train=hyper_align.align(torch.tensor(source_train)).numpy()\n",
    "    aligned_source_comparison=hyper_align.align(torch.tensor(comparison_source)).numpy()\n",
    "    \n",
    "    #adjust values\n",
    "    final_mean=target_test.mean(0)\n",
    "    final_std=target_test.std(0)\n",
    "    \n",
    "    \n",
    "    aligned_source_test_adj=(aligned_source_test-aligned_source_test.mean(0))/(1e-8+aligned_source_test.std(0))\n",
    "    aligned_source_test_adj=final_std*aligned_source_test_adj+final_mean\n",
    "\n",
    "    aligned_source_train_adj=(aligned_source_train-aligned_source_train.mean(0))/(1e-8+aligned_source_train.std(0))\n",
    "    aligned_source_train_adj=final_std*aligned_source_train_adj+final_mean\n",
    "\n",
    "    aligned_source_comparison_adj=(aligned_source_comparison-aligned_source_test.mean(0))/(1e-8+aligned_source_test.std(0))\n",
    "    aligned_source_comparison_adj=final_std*aligned_source_comparison_adj+final_mean\n",
    "    \n",
    "    os.makedirs(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned\",exist_ok=True)\n",
    "\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_train_fmriavg_nsdgeneral_sub{source_idx}_randomhyper_fraction-{k}.npy\",aligned_source_train_adj)\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_test_fmriavg_nsdgeneral_sub{source_idx}_randomhyper_fraction-{k}.npy\",aligned_source_test_adj)\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_comparison_fmriavg_nsdgeneral_sub{source_idx}_randomhyper_fraction-{k}.npy\",aligned_source_comparison_adj)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44a63c-6196-46cc-8dfc-ec8e9b597462",
   "metadata": {},
   "source": [
    "## Full SVD Hyperalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71527da4-54a3-4615-99e5-4e92bc03570b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for k,v in tqdm.tqdm(percent_indices.items()):\n",
    "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "    hyper_align = HyperAlign(device)\n",
    "\n",
    "    source = torch.tensor(source_test[v])\n",
    "    target = torch.tensor(target_test[v])\n",
    "    hyper_align.fit(source,target)\n",
    "\n",
    "    aligned_source_test=hyper_align.align(torch.tensor(source_test)).numpy()\n",
    "    aligned_source_train=hyper_align.align(torch.tensor(source_train)).numpy()\n",
    "    aligned_source_comparison=hyper_align.align(torch.tensor(comparison_source)).numpy()\n",
    "    \n",
    "    #adjust values\n",
    "    final_mean=target_test.mean(0)\n",
    "    final_std=target_test.std(0)\n",
    "    \n",
    "    \n",
    "    aligned_source_test_adj=(aligned_source_test-aligned_source_test.mean(0))/(1e-8+aligned_source_test.std(0))\n",
    "    aligned_source_test_adj=final_std*aligned_source_test_adj+final_mean\n",
    "\n",
    "    aligned_source_train_adj=(aligned_source_train-aligned_source_train.mean(0))/(1e-8+aligned_source_train.std(0))\n",
    "    aligned_source_train_adj=final_std*aligned_source_train_adj+final_mean\n",
    "\n",
    "    aligned_source_comparison_adj=(aligned_source_comparison-aligned_source_test.mean(0))/(1e-8+aligned_source_test.std(0))\n",
    "    aligned_source_comparison_adj=final_std*aligned_source_comparison_adj+final_mean\n",
    "    \n",
    "    os.makedirs(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned\",exist_ok=True)\n",
    "\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_train_fmriavg_nsdgeneral_sub{source_idx}_hyper_fraction-{k}.npy\",aligned_source_train_adj)\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_test_fmriavg_nsdgeneral_sub{source_idx}_hyper_fraction-{k}.npy\",aligned_source_test_adj)\n",
    "    np.save(f\"/home/matteo/data/NSD/processed_roi/subj0{source_idx}/aligned_to_subj0{target_idx}/nsd_comparison_fmriavg_nsdgeneral_sub{source_idx}_hyper_fraction-{k}.npy\",aligned_source_comparison_adj)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bcd929-7cf1-4d6a-ba94-8bdfc6a35bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine\n"
     ]
    }
   ],
   "source": [
    "print(\"fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea071916-d36a-4b45-bb73-16bdab97df90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
